{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192b6679",
   "metadata": {},
   "source": [
    "**Instituto Tecnológico de Aeronáutica – ITA**\n",
    "\n",
    "**Visão Computacional - CM-203**\n",
    "\n",
    "**Professores:** \n",
    "\n",
    "Elcio Hideiti Shiguemori\n",
    "\n",
    "Gabriel Adriano de Melo\n",
    "\n",
    "Marcos Ricardo Omena de Albuquerque Maximo\n",
    "\n",
    "**Orientações padrão:**\n",
    "\n",
    "Antes de você entregar o Lab, tenha certeza de que tudo está rodando corretamente (sequencialmente): Primeiro, **reinicie o kernel** (`Runtime->Restart Runtime` no Colab ou `Kernel->Restart` no Jupyter), depois rode todas as células (`Runtime->Run All` no Colab ou `Cell->Run All` no Jupyter) e verifique que as células rodem sem erros, principalmente as de correção automática que apresentem os `assert`s.\n",
    "\n",
    "É muito importante que vocês não apaguem as células de resposta para preenchimento, isto é, as que contenham o `ESCREVA SEU CÓDIGO AQUI` ou o \"ESCREVA SUA RESPOSTA AQUI\", além das células dos `assert`, pois elas contém metadados com o id da célula para os sistemas de correção automatizada e manual. O sistema de correção automatizada executa todo o código do notebook, adicionando testes extras nas células de teste. Não tem problema vocês criarem mais células, mas não apaguem as células de correção. Mantenham a solução dentro do espaço determinado, por organização. Se por acidente acontecer de apagarem alguma célula que deveria ter a resposta, recomendo iniciar de outro notebook (ou dar um `Undo` se possível), pois não adianta recriar a célula porque perdeu o ID.\n",
    "\n",
    "Os Notebooks foram programados para serem compatíveis com o Google Colab, instalando as dependências necessárias automaticamente a baixando os datasets necessários a cada Lab. Os comandos que se inicial por ! (ponto de exclamação) são de bash e também podem ser executados no terminal linux, que justamente instalam as dependências."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba8d52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a26919",
   "metadata": {},
   "source": [
    "# Laboratório de Atenção Visual\n",
    "\n",
    "Este é um laboratório bem resumido para realizar uma implementação e verificação simples de uma camada de modelo de atenção convolucional, uma cabeça do modelo Transformer e uma inferência por difusão de um modelo generativo. Assim, os principais conceitos da aula serão colocados em prática.\n",
    "\n",
    "É necessário uma GPU com capacidade de computação CUDA para poder executar o modelo do Stable Diffusion. Para o Colab é necessário selecionar a instância de GPU: `Edit > Notebook settings` ou `Runtime>Change runtime type` e selecionar `GPU` como o acelerador de hardware. Para a correção, serão executados apenas os asserts, então não precisa se preocupar se não estiver conseguindo se conectar a uma GPU no Colab, é só comentar o código abaixo do assert do Stable Diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66760b45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a19d54317cf85ae21f5c4d8e60d45a86",
     "grade": false,
     "grade_id": "imports-baixar",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install -Uq diffusers==0.7.2 transformers==4.24.0 fastcore==1.5.27 # opencv-contrib-python==4.6.0.66 torch==1.12.1\n",
    "# O Google Drive bloqueou o download do arquivo grande, mas pode baixar manualmente e mover para a pasta se quiser\n",
    "# Basta depois habilitar local_files_only=True no model.from_pretrained\n",
    "# ! [ ! -d ~/.cache/huggingface ] && mkdir -p ~/.cache/huggingface && gdown -O ~/.cache/models.tar 1S2cvcS-XlNZFam5kLSwfyWFcViA-on80 && tar -xf ~/.cache/models.tar -C ~/.cache/ && rm ~/.cache/models.tar\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler, DDPMScheduler\n",
    "import logging\n",
    "hf_token = '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' # Token para baixar do Hugging Face, se quiser pode mudar\n",
    "executar_transformer = True\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b487247",
   "metadata": {},
   "source": [
    "## Atenção Convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452212b",
   "metadata": {},
   "source": [
    "Um dos principais elementos de atenção em imagens, além da realizada por camadas, é a atenção espacial. Ela é utilizada em diversos modelos convolucionais estado-da-arte.\n",
    "\n",
    "$\\textbf{M}_{s}\\left(F\\right) = \\sigma\\left(f^{7x7}\\left(\\left[\\text{AvgPool}\\left(F\\right);\\text{MaxPool}\\left(F\\right)\\right]\\right)\\right)$\n",
    "\n",
    "$\\textbf{M}_{s}\\left(F\\right) = \\sigma\\left(f^{7x7}\\left(\\left[\\mathbf{F}^{s}_{avg};\\mathbf{F}^{s}_{max} \\right]\\right)\\right)$\n",
    "\n",
    "![Módulo de Atenção Espacial](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_1.27.27_PM_CjrAZaI.png)\n",
    "\n",
    "Implemente a função abaixo relativa ao módulo de atenção espacial (SAM, Spatial Attention Module) do modelo CBAM de [Woo et. al (2018)](https://arxiv.org/abs/1807.06521). (3 pontos)\n",
    "\n",
    "Utilize as operações `torch.cat` para concatenar dois ou mais tensores, `torch.max` para calcular o valor máximo ao longo de uma dimensão especificada, `torch.mean` para calcular o valor médio ao longo de uma dimensão, `torch.sigmoid` para aplicar a função sigmóide a cada elemento do tensor, e `.unsqueeze`/`.view`/`.reshape`/`tensor[:, None, :, :]` para modificar o shape do tensor. Multiplicação elemento a elemento (com broadcasting) é realizado pelo `*` ou `torch.multiply`.\n",
    "\n",
    "Para invocar a operação, basta realizar uma chamada de função `operacao(entrada)`. \n",
    "<details><summary><b>---Dica---</b></summary>\n",
    "<p>\n",
    "A média e o valor máximo é calculado sobre os canais, em um tensor no formato (N, C, H, W) isso corresponde à dimensão 1 (C), resultando em um tensor de (N, H, W). Para poder fazer o broadcasting depois é necessário que haja o casamento das dimensões, e que o vetor tenha o formato de (N, 1, H, W). Assim basta fazer um reshape adequado.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e707af5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1825cdcaea9d2e7031045dea91c5d100",
     "grade": false,
     "grade_id": "atencao-conv",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def atencao_espacial(operacao_convolucional, features):\n",
    "    \"\"\"\n",
    "    Implementa o módulo de atenção espacial (SAM) aplicando o mapa de atenção sobre \n",
    "    as features de forma multiplicativa. OBS: concatena o Avg antes do Max, nessa ordem.\n",
    "    \n",
    "    Args:\n",
    "        operacao_convolucional (nn.Module): Operação de convolução (filtro f) que recebe dois canais de entrada\n",
    "            e retorna apenas um canal de saída, mantendo a largura e a altura do tensor.\n",
    "        features (torch.tensor): Tensor de tamanho (N, C, H, W), onde N é o número de batches, C é o número de \n",
    "            canais das features, H é a altura do tensor de features e W, sua largura.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor: Tensor de tamanho (N, C, H, W) resultado final da operação de atenção sobre as features\n",
    "    \"\"\"\n",
    "    N, C, H, W = features.shape\n",
    "    resultado = features*torch.sigmoid(operacao_convolucional(\n",
    "        torch.cat((torch.mean(features, dim=1), torch.max(features, dim=1)[0]))\n",
    "    ))[:, None, :, :]\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144a288",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e6e97683417f47596b07c3506ac6cd7",
     "grade": true,
     "grade_id": "testa-atencao-conv",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "conv_teste = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "conv_teste.load_state_dict(OrderedDict([('weight', torch.tensor([[[ [-0.0165, -0.0119, -0.1987],\n",
    "                                                                    [ 0.1247, -0.0875,  0.0907],\n",
    "                                                                    [-0.1658,  0.2204,  0.1959]],\n",
    "\n",
    "                                                                   [[-0.0925, -0.1122, -0.2284],\n",
    "                                                                    [ 0.0141,  0.0105, -0.0061],\n",
    "                                                                    [ 0.0526,  0.1581, -0.1757]] ]])),\n",
    "                                        ('bias', torch.tensor([0.0675]))]))\n",
    "features_testa = torch.tensor([[[[-0.0165, -0.0119, -0.1987, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [-0.1658,  0.2204,  0.1959, 1, 1, 1, 1]],\n",
    "                                [[-0.0165, -0.0119, -0.1987, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [-0.1658,  0.2204,  0.1959, 1, 1, 1, 1]],\n",
    "                                [[-0.0165, -0.0119, -0.1987, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [-0.1658,  0.2204,  0.1959, 1, 1, 1, 1]]\n",
    "                              ]])\n",
    "resultado_testa = atencao_espacial(conv_teste, features_testa)\n",
    "assert resultado_testa.requires_grad\n",
    "assert torch.norm(resultado_testa - torch.tensor(\n",
    "       [[[[-0.0087, -0.0060, -0.1107,  0.6073,  0.6223,  0.6223,  0.5974],\n",
    "          [ 0.0656, -0.0469,  0.0407,  0.4866,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0660, -0.0440,  0.0401,  0.4788,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0628, -0.0473,  0.0402,  0.4758,  0.4599,  0.4599,  0.5403],\n",
    "          [-0.0879,  0.1103,  0.0852,  0.3873,  0.3902,  0.3902,  0.4741]],\n",
    "\n",
    "         [[-0.0087, -0.0060, -0.1107,  0.6073,  0.6223,  0.6223,  0.5974],\n",
    "          [ 0.0656, -0.0469,  0.0407,  0.4866,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0660, -0.0440,  0.0401,  0.4788,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0628, -0.0473,  0.0402,  0.4758,  0.4599,  0.4599,  0.5403],\n",
    "          [-0.0879,  0.1103,  0.0852,  0.3873,  0.3902,  0.3902,  0.4741]],\n",
    "\n",
    "         [[-0.0087, -0.0060, -0.1107,  0.6073,  0.6223,  0.6223,  0.5974],\n",
    "          [ 0.0656, -0.0469,  0.0407,  0.4866,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0660, -0.0440,  0.0401,  0.4788,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0628, -0.0473,  0.0402,  0.4758,  0.4599,  0.4599,  0.5403],\n",
    "          [-0.0879,  0.1103,  0.0852,  0.3873,  0.3902,  0.3902,  0.4741]]]])).item() < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1bdde",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7386c63",
   "metadata": {},
   "source": [
    "Um modelo de atenção inicialmente proposto para Processamento de Linguagem Natural (NLP) a fim de permitir operações altamente paralelizáveis e com maior facilidade de treinar sequências mais longas. Também já foi adaptado para Visão Computacional através do ViT (Vision Transformers), que essencialmente tratam a imagem como uma sequência de pixels.\n",
    "\n",
    "![Arquitetura Transformer](https://lilianweng.github.io/posts/2018-06-24-attention/transformer.png)\n",
    "\n",
    "Foram propostos inicialmente por [Vaswani et. al (2017)](https://arxiv.org/abs/1706.03762v5), segundo as equações abaixo:\n",
    "\n",
    "$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{n}})\\mathbf{V}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= [\\text{head}_1; \\dots; \\text{head}_h]\\mathbf{W}^O \\\\\n",
    "\\text{where head}_i &= \\text{Attention}(\\mathbf{Q}\\mathbf{W}^Q_i, \\mathbf{K}\\mathbf{W}^K_i, \\mathbf{V}\\mathbf{W}^V_i)\n",
    "\\end{aligned}$\n",
    "\n",
    "Implemente a cabeça de atenção do transformer. (4 pontos)\n",
    "\n",
    "Use a função `.transpose(dim_1, dim_2)` para transpor as dimesões dim_1 e dim_2 de um tensor. Para a multiplicação matricial sobre as últimas duas dimensões de um tensor use o operador`@` ou a função `torch.matmul`. Para obter as dimensões de um tensor use o atributo `.shape` ou a função `.size()`.\n",
    "\n",
    "<details><summary><b>Dica</b></summary>\n",
    "<p>\n",
    "Utilize tensor.transpose(-1, -2) para transpor as duas últimas dimensões de um tensor\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19c84c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d04b0149e170273cb2fddb020d977e8",
     "grade": false,
     "grade_id": "transformer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cabeca_transformer(q, k, v, proj_q, proj_k, proj_v, reproj, softmax):\n",
    "    \"\"\"\n",
    "    Implementa a cabeça da arquitetura Transformer dada pela equação \n",
    "    (softmax((q' @ k'.T) / sqrt(D_k)) @ v')' onde ' é a operação de projeção linear para cada uma dos \n",
    "    tensores no seu espaço vetorial adequado. Novamente, as operações, conforme exercício anterior, \n",
    "    podem ser utilizadas a partir de uma chamada de função.\n",
    "    \n",
    "    Args:\n",
    "        q (torch.tensor): Tensor de query de tamanho (N, L, D)\n",
    "        k (torch.tensor): Tensor de key de tamanho (N, L, D)\n",
    "        v (torch.tensor): Tensor de value de tamanho (N, L, D)\n",
    "        proj_q (nn.Module): Operação que realiza a projeção linear de um tensor (N, L, D) em \n",
    "            H espaços vetoriais de dimensão D_k, resultando em um tensor (N, H, L, D_k)\n",
    "        proj_k (nn.Module): Operação que realiza a projeção linear de um tensor (N, L, D) em\n",
    "             H espaços vetoriais de dimensão D_k, resultando em um tensor (N, H, L, D_k)\n",
    "        proj_v (nn.Module): Operação que realiza a projeção linear de um tensor (N, L, D) em\n",
    "             H espaços vetoriais de dimensão D_v, resultando em um tensor (N, H, L, D_v)\n",
    "        reproj (nn.Module): Operação que realiza a concatenação e projeção linear de um tensor \n",
    "             (N, H, L, D_v) de dimensão E2, resultando em um tensor (N, L, D)\n",
    "        softmax (nn.Module): Operação de softmax sobre a última dimensão do tensor.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor: Tensor de tamanho (N, L, D) resultado final da operação do transformer sobre q, k, v\n",
    "    \"\"\"\n",
    "    _, _, _, D_k = proj_k(k).shape\n",
    "    resultado = reproj(softmax((proj_q(q) @ proj_k(k).transpose(-1, -2)) / np.sqrt(D_k)) @ proj_v(v))\n",
    "    return resultado\n",
    "\n",
    "class Projeta(nn.Linear):\n",
    "    def __init__(self, dim_in, quant_projs, dim_out, **kwargs):\n",
    "        super().__init__(dim_in, quant_projs * dim_out, **kwargs)\n",
    "        self.quant_projs = quant_projs\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_flat = super().forward(x)\n",
    "        return x_flat.reshape(*x_flat.shape[:2], self.quant_projs, -1).transpose(1, 2)\n",
    "\n",
    "class ReProjeta(nn.Linear):\n",
    "    def __init__(self, dim_in, quant_projs, dim_out, **kwargs):\n",
    "        super().__init__(dim_in * quant_projs, dim_out, **kwargs)\n",
    "        self.quant_projs = quant_projs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_flat = x.transpose(1, 2).contiguous()\n",
    "        return super().forward(x_flat.reshape(*x_flat.shape[:2], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a6b90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2570b4798e6cd75aae610a8b6a03e4df",
     "grade": true,
     "grade_id": "testa-transformer",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=-1)\n",
    "proj_q = Projeta(4, 2, 3)\n",
    "proj_k = Projeta(4, 2, 3)\n",
    "proj_v = Projeta(4, 2, 5)\n",
    "reproj = ReProjeta(5, 2, 4)\n",
    "proj_q.load_state_dict(OrderedDict([('weight', torch.tensor([[-0.2903,  0.1144, -0.2388,  0.3808],\n",
    "                      [ 0.4571, -0.1722, -0.3059,  0.0529],\n",
    "                      [-0.0764,  0.4670, -0.2218, -0.3888],\n",
    "                      [ 0.4820, -0.0895,  0.0496, -0.4707],\n",
    "                      [ 0.0030, -0.0348,  0.4132,  0.3539],\n",
    "                      [-0.2953,  0.2528,  0.2744, -0.0833]])),\n",
    "    ('bias', torch.tensor([-0.2345, -0.0744,  0.1075, -0.1458, -0.4157, -0.3114]))]))\n",
    "proj_k.load_state_dict(OrderedDict([('weight', torch.tensor([[ 0.2761, -0.4321,  0.3839,  0.1454],\n",
    "                      [-0.3980,  0.1969, -0.4166, -0.2317],\n",
    "                      [-0.1690, -0.1395,  0.3167, -0.3027],\n",
    "                      [-0.1422, -0.2583, -0.4430, -0.0448],\n",
    "                      [ 0.4761,  0.1354,  0.1436,  0.3219],\n",
    "                      [ 0.3956,  0.0885,  0.2519,  0.1227]])),\n",
    "    ('bias', torch.tensor([0.4507, 0.3675, 0.3032, 0.3873, 0.4495, 0.1289]))]))\n",
    "proj_v.load_state_dict(OrderedDict([('weight', torch.tensor([[ 0.2719, -0.0664,  0.3742,  0.3409],\n",
    "                      [ 0.1177,  0.4588, -0.3498, -0.4507],\n",
    "                      [-0.4851, -0.3594,  0.2124, -0.1817],\n",
    "                      [-0.3162, -0.1503, -0.1955,  0.3816],\n",
    "                      [ 0.0005, -0.0776, -0.4964, -0.1608],\n",
    "                      [ 0.0581, -0.1783, -0.2951,  0.0964],\n",
    "                      [-0.3771,  0.1194, -0.4692,  0.1051],\n",
    "                      [-0.4773, -0.0826,  0.4722, -0.2247],\n",
    "                      [-0.1419,  0.0064,  0.3859,  0.1678],\n",
    "                      [ 0.2845, -0.4944,  0.4023, -0.2722]])),\n",
    "('bias', torch.tensor([ 0.1750,  0.1422, -0.3162,  0.2938,  0.0050, -0.0249,  0.2706, -0.2545, 0.0081, -0.2179]))]))\n",
    "reproj.load_state_dict(OrderedDict([('weight',\n",
    "    torch.tensor([[ 0.2090, -0.1364,  0.2534,  0.0466, -0.1310,  0.1216,  0.0299, -0.2586, 0.3095, -0.2708],\n",
    "            [-0.2939,  0.2829, -0.0410,  0.2643,  0.0008, -0.3008,  0.2150,  0.0737, -0.0611, -0.0701],\n",
    "            [ 0.2135, -0.1298, -0.3017,  0.2684, -0.0917, -0.1428,  0.1363, -0.1012, 0.2472,  0.2877],\n",
    "            [ 0.1128,  0.0359, -0.1215,  0.2214,  0.2173, -0.1789,  0.1038,  0.0059, 0.2911, -0.0398]])),\n",
    "             ('bias', torch.tensor([ 0.2898,  0.0237, -0.2518, -0.2610]))]))\n",
    "sequencia = torch.tensor([[\n",
    "                   [ 0.1177,  0.4588, -0.3498, -0.4507],\n",
    "                   [-0.0764,  0.4670, -0.2218, -0.3888],\n",
    "                   [-0.3771,  0.1194, -0.4692,  0.1051],\n",
    "                   [-0.1419,  0.0064,  0.3859,  0.1678],\n",
    "                   [-0.3162, -0.1503, -0.1955,  0.3816],\n",
    "                   ]])\n",
    "assert torch.norm(cabeca_transformer(sequencia, sequencia, sequencia, proj_q, proj_k, proj_v, reproj, softmax) -\\\n",
    "    torch.tensor([[[ 0.3581,  0.3061, -0.1407, -0.0638],\n",
    "                   [ 0.3595,  0.3037, -0.1386, -0.0639],\n",
    "                   [ 0.3618,  0.3005, -0.1352, -0.0635],\n",
    "                   [ 0.3583,  0.3030, -0.1361, -0.0625],\n",
    "                   [ 0.3604,  0.3014, -0.1349, -0.0626]]])).item() < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0fc79",
   "metadata": {},
   "source": [
    "## Modelo de Difusão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b17d7b",
   "metadata": {},
   "source": [
    "![Exemplos Gerados](https://techcrunch.com/wp-content/uploads/2022/08/53118410-9cce-468a-8bf6-1b8ce4dd1390_1600x925.webp?resize=1200,694)\n",
    "\n",
    "Os modelos já baixados no drive foram distribuídos pelo [HuggingFace](https://huggingface.co/) e o exemplo abaixo pelo [FastAI](https://github.com/fastai/diffusion-nbs/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniza_codifica_string(lista_strings, tokenizer, text_encoder):\n",
    "    ml = tokenizer.model_max_length\n",
    "    inp = tokenizer(lista_strings, padding=\"max_length\", max_length=ml, truncation=True, return_tensors=\"pt\")\n",
    "    codificado = text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n",
    "    vaz = tokenizer([\"\"] * len(lista_strings), padding=\"max_length\", max_length=ml, truncation=True, return_tensors=\"pt\")\n",
    "    vazio = text_encoder(vaz.input_ids.to(\"cuda\"))[0].half()\n",
    "    return torch.cat([vazio, codificado])\n",
    "\n",
    "def mostra_imagem(espaco_latente, vae, indice=0):\n",
    "    with torch.no_grad():\n",
    "        imagem_normalizada = vae.decode(1 / 0.18215 * espaco_latente).sample[indice]\n",
    "    image = (imagem_normalizada/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n",
    "    return Image.fromarray((image*255).round().astype(\"uint8\"))\n",
    "\n",
    "def loop_difusao(texto_codificado, unet, escalonador, steps=70, g=7.5, width=512, height=512, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    escalonador.set_timesteps(steps)\n",
    "    espaco_latente = torch.randn((texto_codificado.shape[0]//2, unet.in_channels, height//8, width//8))\n",
    "    espaco_latente = espaco_latente.to(\"cuda\").half() * escalonador.init_noise_sigma\n",
    "    for step in tqdm(escalonador.timesteps):\n",
    "        inp = escalonador.scale_model_input(torch.cat([espaco_latente] * 2), step)\n",
    "        with torch.no_grad():\n",
    "            u, t = unet(inp, step, encoder_hidden_states=texto_codificado).sample.chunk(2)\n",
    "        ruido_estimado = u + g*(t-u)\n",
    "        espaco_latente = escalonador.step(ruido_estimado, step, espaco_latente).prev_sample\n",
    "    return espaco_latente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca21362",
   "metadata": {},
   "source": [
    "Observe a implementação das funções acima, que realizam a tokenização e a codificação das entradas textuais, a conversão do espaço latente para uma imagem em pixels RGB e, principalmente, o loop de difusão, no qual o espaço latente é iterativamente atualizado, diminuindo-se o ruído, que é estimado pelo modelo a cada iteração.\n",
    "\n",
    "Investigue o modelo do Stable Diffusion gerando a sua própria string para guiar a difusão. Sugiro fortemente que seja em inglês, que é a principal linguagem que o codificador de texto (CLIP) foi treinado. (1 ponto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31376d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "445484a4609549d5a3256c5690383c0d",
     "grade": false,
     "grade_id": "difusao",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gerar_entrada_textual():\n",
    "    \"\"\"\n",
    "    Retorna uma string de entrada parar o modelo de Difusão\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    Returns:\n",
    "        str: String para guiar a difusão\n",
    "    \"\"\"\n",
    "    string_entrada = \"Pikachu eating an hamburger in Paris\"\n",
    "    return string_entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e1455",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "211c9776e20e574ab9e2cd70a37378ee",
     "grade": true,
     "grade_id": "testa-difusao",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "string_entrada = gerar_entrada_textual()\n",
    "assert string_entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88375f",
   "metadata": {},
   "source": [
    "E agora veja a execução do Stable Diffusion com a sua entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71685dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert executar_transformer # Esse assert não será corrigido\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", use_auth_token=hf_token, torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", use_auth_token=hf_token, torch_dtype=torch.float16).to(\"cuda\")\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", use_auth_token=hf_token, torch_dtype=torch.float16).to(\"cuda\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=hf_token, subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert executar_transformer # Esse assert não será corrigido\n",
    "escalonador = LMSDiscreteScheduler(beta_start=0.00085,beta_end=0.012,beta_schedule=\"scaled_linear\",num_train_timesteps=1000)\n",
    "texto_codificado = tokeniza_codifica_string([string_entrada], tokenizer, text_encoder)\n",
    "imagens_geradas = loop_difusao(texto_codificado, unet, escalonador, steps=70, seed=42)\n",
    "mostra_imagem(imagens_geradas, vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca032da",
   "metadata": {},
   "source": [
    "Implemente o loop abaixo de difusão com os coeficientes de atualização simples, no qual a imagem original é recuperada pela subtração do seu respectivo ruído, ambos ponderados por fatores. (2 pontos)\n",
    "\n",
    "Por ser um modelo mais simples de escalonamento, o DDPM (Denoising diffusion probabilistic models) precisa de mais passos de difusão, cada um com menor amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe29ed",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0cfeee9f91a63b3e7a04d880e64d070",
     "grade": false,
     "grade_id": "escalonador-simples",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def atualizacao_simples(ruido_estimado, imagem_latente, fator_ruido, fator_imagem):\n",
    "    \"\"\"\n",
    "    Implementa uma simples atualização do escalonador de forma a remover o ruído presente na imagem latente.\n",
    "    O ruído é multiplicado pelo fator beta e a imagem pelo fator alfa, que dependem do passo de difusão.\n",
    "    \n",
    "    Args:\n",
    "        ruido_estimado (torch.tensor): Tensor de ruído no espaço latente de tamanho (N, C, H, W)\n",
    "        imagem_latente (torch.tensor): Tensor de imagem no espaço latente de tamanho (N, C, H, W)\n",
    "        fator_ruido (float): Fator que deve multiplicar o ruído\n",
    "        fator_imagem (float): Fator que deve multiplicar a imagem latente\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor: Tensor de tamanho (N, C, H, W) estimado a partir da remoção do ruído da imagem, poderados\n",
    "            por seus respectivos fatores.\n",
    "    \"\"\"\n",
    "    imagem_latente_reconstruida = fator_imagem*imagem_latente - fator_ruido*ruido_estimado\n",
    "    return imagem_latente_reconstruida\n",
    "\n",
    "def loop_difusao_simples(texto_codificado, unet, steps=70, g=7.5, width=512, height=512, seed=42, train_steps=1000, beta_start=0.00085, beta_end=0.012):\n",
    "    torch.manual_seed(seed)\n",
    "    betas = np.linspace(beta_start, beta_end, train_steps)\n",
    "    alfas = 1.0 - betas\n",
    "    alfa_prod = np.pad(np.cumprod(alfas, axis=0), (0, 1), constant_values=1)\n",
    "    beta_prod = 1 - alfa_prod\n",
    "    espaco_latente = torch.randn((texto_codificado.shape[0]//2, unet.in_channels, height//8, width//8)).to(\"cuda\").half()\n",
    "    for t in tqdm(np.arange(0, train_steps, train_steps // steps)[::-1]):\n",
    "        inp = torch.cat([espaco_latente] * 2)\n",
    "        with torch.no_grad():\n",
    "            uncond, text = unet(inp, t, encoder_hidden_states=texto_codificado).sample.chunk(2)\n",
    "        ruido_estimado = uncond + g*(text-uncond)\n",
    "        espaco_latente_original = torch.clamp(atualizacao_simples(ruido_estimado, espaco_latente, \n",
    "                                beta_prod[t]**0.5/alfa_prod[t]**0.5, 1/alfa_prod[t]**0.5), -1, 1)\n",
    "        coeff_original = alfa_prod[t-1]**0.5 * betas[t]/beta_prod[t]\n",
    "        coeff_atual = alfas[t]**0.5 * beta_prod[t-1]/beta_prod[t]\n",
    "        espaco_latente = coeff_original*espaco_latente_original + coeff_atual*espaco_latente\n",
    "        variancia = np.clip(beta_prod[t-1]/beta_prod[t] * betas[t], 1e-20, None)\n",
    "        ruido = torch.randn(ruido_estimado.shape, device=ruido_estimado.device, dtype=ruido_estimado.dtype)\n",
    "        espaco_latente = espaco_latente + variancia**0.5 * ruido\n",
    "    return espaco_latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdfcf0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb953bd38e7326b1f6d833665860c822",
     "grade": true,
     "grade_id": "testa-escalonador-simples",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert torch.norm(atualizacao_simples(torch.tensor([[[[0.1, -0.1], [0.2,-0.2]]]]), \n",
    "                                      torch.tensor([[[[2, 3], [4,3]]]]), 0.4, 0.9) - \\\n",
    "                  torch.tensor([[[[1.76, 2.74],[3.52, 2.78]]]])) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5b804",
   "metadata": {},
   "source": [
    "Apesar do nome de simples, e de ser a mais simples que realmente é empregada durante o treino do modelo, a DDPM requer uma quantidade de iterações que justamente de aproxima do treino, no caso, mil passos, para que tenha uma convergência adequada. Por isso o outro método acaba sendo mais de uma ordem de grandeza mais rápido para inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert executar_transformer # Esse assert não será corrigido\n",
    "imagens_geradas = loop_difusao_simples(texto_codificado, unet, steps=1000, seed=42)\n",
    "mostra_imagem(imagens_geradas, vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159d3e3",
   "metadata": {},
   "source": [
    "Sinta-se a vontade para explorar o modelo e outros tipos de entradas textuais."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "6eb7e7de5c7beb0676f30b094e7a91c2f640a81022667a282200c880693dc871"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
