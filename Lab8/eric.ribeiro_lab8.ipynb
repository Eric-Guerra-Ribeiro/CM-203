{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192b6679",
   "metadata": {},
   "source": [
    "**Instituto Tecnol√≥gico de Aeron√°utica ‚Äì ITA**\n",
    "\n",
    "**Vis√£o Computacional - CM-203**\n",
    "\n",
    "**Professores:** \n",
    "\n",
    "Elcio Hideiti Shiguemori\n",
    "\n",
    "Gabriel Adriano de Melo\n",
    "\n",
    "Marcos Ricardo Omena de Albuquerque Maximo\n",
    "\n",
    "**Orienta√ß√µes padr√£o:**\n",
    "\n",
    "Antes de voc√™ entregar o Lab, tenha certeza de que tudo est√° rodando corretamente (sequencialmente): Primeiro, **reinicie o kernel** (`Runtime->Restart Runtime` no Colab ou `Kernel->Restart` no Jupyter), depois rode todas as c√©lulas (`Runtime->Run All` no Colab ou `Cell->Run All` no Jupyter) e verifique que as c√©lulas rodem sem erros, principalmente as de corre√ß√£o autom√°tica que apresentem os `assert`s.\n",
    "\n",
    "√â muito importante que voc√™s n√£o apaguem as c√©lulas de resposta para preenchimento, isto √©, as que contenham o `ESCREVA SEU C√ìDIGO AQUI` ou o \"ESCREVA SUA RESPOSTA AQUI\", al√©m das c√©lulas dos `assert`, pois elas cont√©m metadados com o id da c√©lula para os sistemas de corre√ß√£o automatizada e manual. O sistema de corre√ß√£o automatizada executa todo o c√≥digo do notebook, adicionando testes extras nas c√©lulas de teste. N√£o tem problema voc√™s criarem mais c√©lulas, mas n√£o apaguem as c√©lulas de corre√ß√£o. Mantenham a solu√ß√£o dentro do espa√ßo determinado, por organiza√ß√£o. Se por acidente acontecer de apagarem alguma c√©lula que deveria ter a resposta, recomendo iniciar de outro notebook (ou dar um `Undo` se poss√≠vel), pois n√£o adianta recriar a c√©lula porque perdeu o ID.\n",
    "\n",
    "Os Notebooks foram programados para serem compat√≠veis com o Google Colab, instalando as depend√™ncias necess√°rias automaticamente a baixando os datasets necess√°rios a cada Lab. Os comandos que se inicial por ! (ponto de exclama√ß√£o) s√£o de bash e tamb√©m podem ser executados no terminal linux, que justamente instalam as depend√™ncias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba8d52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a26919",
   "metadata": {},
   "source": [
    "# Laborat√≥rio de Aten√ß√£o Visual\n",
    "\n",
    "Este √© um laborat√≥rio bem resumido para realizar uma implementa√ß√£o e verifica√ß√£o simples de uma camada de modelo de aten√ß√£o convolucional, uma cabe√ßa do modelo Transformer e uma infer√™ncia por difus√£o de um modelo generativo. Assim, os principais conceitos da aula ser√£o colocados em pr√°tica.\n",
    "\n",
    "√â necess√°rio uma GPU com capacidade de computa√ß√£o CUDA para poder executar o modelo do Stable Diffusion. Para o Colab √© necess√°rio selecionar a inst√¢ncia de GPU: `Edit > Notebook settings` ou `Runtime>Change runtime type` e selecionar `GPU` como o acelerador de hardware. Para a corre√ß√£o, ser√£o executados apenas os asserts, ent√£o n√£o precisa se preocupar se n√£o estiver conseguindo se conectar a uma GPU no Colab, √© s√≥ comentar o c√≥digo abaixo do assert do Stable Diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66760b45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a19d54317cf85ae21f5c4d8e60d45a86",
     "grade": false,
     "grade_id": "imports-baixar",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install -Uq diffusers==0.7.2 transformers==4.24.0 fastcore==1.5.27 # opencv-contrib-python==4.6.0.66 torch==1.12.1\n",
    "# O Google Drive bloqueou o download do arquivo grande, mas pode baixar manualmente e mover para a pasta se quiser\n",
    "# Basta depois habilitar local_files_only=True no model.from_pretrained\n",
    "#! [ ! -d ~/.cache/huggingface ] && mkdir -p ~/.cache/huggingface && gdown -O ~/.cache/models.tar 1S2cvcS-XlNZFam5kLSwfyWFcViA-on80 && tar -xf ~/.cache/models.tar -C ~/.cache/ && rm ~/.cache/models.tar\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler, DDPMScheduler\n",
    "import logging\n",
    "hf_token = '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' # Token para baixar do Hugging Face, se quiser pode mudar\n",
    "executar_transformer = True\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b487247",
   "metadata": {},
   "source": [
    "## Aten√ß√£o Convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452212b",
   "metadata": {},
   "source": [
    "Um dos principais elementos de aten√ß√£o em imagens, al√©m da realizada por camadas, √© a aten√ß√£o espacial. Ela √© utilizada em diversos modelos convolucionais estado-da-arte.\n",
    "\n",
    "$\\textbf{M}_{s}\\left(F\\right) = \\sigma\\left(f^{7x7}\\left(\\left[\\text{AvgPool}\\left(F\\right);\\text{MaxPool}\\left(F\\right)\\right]\\right)\\right)$\n",
    "\n",
    "$\\textbf{M}_{s}\\left(F\\right) = \\sigma\\left(f^{7x7}\\left(\\left[\\mathbf{F}^{s}_{avg};\\mathbf{F}^{s}_{max} \\right]\\right)\\right)$\n",
    "\n",
    "![M√≥dulo de Aten√ß√£o Espacial](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-25_at_1.27.27_PM_CjrAZaI.png)\n",
    "\n",
    "Implemente a fun√ß√£o abaixo relativa ao m√≥dulo de aten√ß√£o espacial (SAM, Spatial Attention Module) do modelo CBAM de [Woo et. al (2018)](https://arxiv.org/abs/1807.06521). (3 pontos)\n",
    "\n",
    "Utilize as opera√ß√µes `torch.cat` para concatenar dois ou mais tensores, `torch.max` para calcular o valor m√°ximo ao longo de uma dimens√£o especificada, `torch.mean` para calcular o valor m√©dio ao longo de uma dimens√£o, `torch.sigmoid` para aplicar a fun√ß√£o sigm√≥ide a cada elemento do tensor, e `.unsqueeze`/`.view`/`.reshape`/`tensor[:, None, :, :]` para modificar o shape do tensor. Multiplica√ß√£o elemento a elemento (com broadcasting) √© realizado pelo `*` ou `torch.multiply`.\n",
    "\n",
    "Para invocar a opera√ß√£o, basta realizar uma chamada de fun√ß√£o `operacao(entrada)`. \n",
    "<details><summary><b>---Dica---</b></summary>\n",
    "<p>\n",
    "A m√©dia e o valor m√°ximo √© calculado sobre os canais, em um tensor no formato (N, C, H, W) isso corresponde √† dimens√£o 1 (C), resultando em um tensor de (N, H, W). Para poder fazer o broadcasting depois √© necess√°rio que haja o casamento das dimens√µes, e que o vetor tenha o formato de (N, 1, H, W). Assim basta fazer um reshape adequado.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e707af5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1825cdcaea9d2e7031045dea91c5d100",
     "grade": false,
     "grade_id": "atencao-conv",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def atencao_espacial(operacao_convolucional, features):\n",
    "    \"\"\"\n",
    "    Implementa o m√≥dulo de aten√ß√£o espacial (SAM) aplicando o mapa de aten√ß√£o sobre \n",
    "    as features de forma multiplicativa. OBS: concatena o Avg antes do Max, nessa ordem.\n",
    "    \n",
    "    Args:\n",
    "        operacao_convolucional (nn.Module): Opera√ß√£o de convolu√ß√£o (filtro ùëì) que recebe dois canais de entrada\n",
    "            e retorna apenas um canal de sa√≠da, mantendo a largura e a altura do tensor.\n",
    "        features (torch.tensor): Tensor de tamanho (N, C, H, W), onde N √© o n√∫mero de batches, C √© o n√∫mero de \n",
    "            canais das features, H √© a altura do tensor de features e W, sua largura.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor: Tensor de tamanho (N, C, H, W) resultado final da opera√ß√£o de aten√ß√£o sobre as features\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144a288",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e6e97683417f47596b07c3506ac6cd7",
     "grade": true,
     "grade_id": "testa-atencao-conv",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "conv_teste = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "conv_teste.load_state_dict(OrderedDict([('weight', torch.tensor([[[ [-0.0165, -0.0119, -0.1987],\n",
    "                                                                    [ 0.1247, -0.0875,  0.0907],\n",
    "                                                                    [-0.1658,  0.2204,  0.1959]],\n",
    "\n",
    "                                                                   [[-0.0925, -0.1122, -0.2284],\n",
    "                                                                    [ 0.0141,  0.0105, -0.0061],\n",
    "                                                                    [ 0.0526,  0.1581, -0.1757]] ]])),\n",
    "                                        ('bias', torch.tensor([0.0675]))]))\n",
    "features_testa = torch.tensor([[[[-0.0165, -0.0119, -0.1987, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [-0.1658,  0.2204,  0.1959, 1, 1, 1, 1]],\n",
    "                                [[-0.0165, -0.0119, -0.1987, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [-0.1658,  0.2204,  0.1959, 1, 1, 1, 1]],\n",
    "                                [[-0.0165, -0.0119, -0.1987, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [ 0.1247, -0.0875,  0.0907, 1, 1, 1, 1],\n",
    "                                 [-0.1658,  0.2204,  0.1959, 1, 1, 1, 1]]\n",
    "                              ]])\n",
    "resultado_testa = atencao_espacial(conv_teste, features_testa)\n",
    "assert resultado_testa.requires_grad\n",
    "assert torch.norm(resultado_testa - torch.tensor(\n",
    "       [[[[-0.0087, -0.0060, -0.1107,  0.6073,  0.6223,  0.6223,  0.5974],\n",
    "          [ 0.0656, -0.0469,  0.0407,  0.4866,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0660, -0.0440,  0.0401,  0.4788,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0628, -0.0473,  0.0402,  0.4758,  0.4599,  0.4599,  0.5403],\n",
    "          [-0.0879,  0.1103,  0.0852,  0.3873,  0.3902,  0.3902,  0.4741]],\n",
    "\n",
    "         [[-0.0087, -0.0060, -0.1107,  0.6073,  0.6223,  0.6223,  0.5974],\n",
    "          [ 0.0656, -0.0469,  0.0407,  0.4866,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0660, -0.0440,  0.0401,  0.4788,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0628, -0.0473,  0.0402,  0.4758,  0.4599,  0.4599,  0.5403],\n",
    "          [-0.0879,  0.1103,  0.0852,  0.3873,  0.3902,  0.3902,  0.4741]],\n",
    "\n",
    "         [[-0.0087, -0.0060, -0.1107,  0.6073,  0.6223,  0.6223,  0.5974],\n",
    "          [ 0.0656, -0.0469,  0.0407,  0.4866,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0660, -0.0440,  0.0401,  0.4788,  0.4599,  0.4599,  0.5403],\n",
    "          [ 0.0628, -0.0473,  0.0402,  0.4758,  0.4599,  0.4599,  0.5403],\n",
    "          [-0.0879,  0.1103,  0.0852,  0.3873,  0.3902,  0.3902,  0.4741]]]])).item() < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1bdde",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7386c63",
   "metadata": {},
   "source": [
    "Um modelo de aten√ß√£o inicialmente proposto para Processamento de Linguagem Natural (NLP) a fim de permitir opera√ß√µes altamente paraleliz√°veis e com maior facilidade de treinar sequ√™ncias mais longas. Tamb√©m j√° foi adaptado para Vis√£o Computacional atrav√©s do ViT (Vision Transformers), que essencialmente tratam a imagem como uma sequ√™ncia de pixels.\n",
    "\n",
    "![Arquitetura Transformer](https://lilianweng.github.io/posts/2018-06-24-attention/transformer.png)\n",
    "\n",
    "Foram propostos inicialmente por [Vaswani et. al (2017)](https://arxiv.org/abs/1706.03762v5), segundo as equa√ß√µes abaixo:\n",
    "\n",
    "$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{n}})\\mathbf{V}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= [\\text{head}_1; \\dots; \\text{head}_h]\\mathbf{W}^O \\\\\n",
    "\\text{where head}_i &= \\text{Attention}(\\mathbf{Q}\\mathbf{W}^Q_i, \\mathbf{K}\\mathbf{W}^K_i, \\mathbf{V}\\mathbf{W}^V_i)\n",
    "\\end{aligned}$\n",
    "\n",
    "Implemente a cabe√ßa de aten√ß√£o do transformer. (4 pontos)\n",
    "\n",
    "Use a fun√ß√£o `.transpose(dim_1, dim_2)` para transpor as dimes√µes dim_1 e dim_2 de um tensor. Para a multiplica√ß√£o matricial sobre as √∫ltimas duas dimens√µes de um tensor use o operador`@` ou a fun√ß√£o `torch.matmul`. Para obter as dimens√µes de um tensor use o atributo `.shape` ou a fun√ß√£o `.size()`.\n",
    "\n",
    "<details><summary><b>Dica</b></summary>\n",
    "<p>\n",
    "Utilize tensor.transpose(-1, -2) para transpor as duas √∫ltimas dimens√µes de um tensor\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19c84c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d04b0149e170273cb2fddb020d977e8",
     "grade": false,
     "grade_id": "transformer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cabeca_transformer(q, k, v, proj_q, proj_k, proj_v, reproj, softmax):\n",
    "    \"\"\"\n",
    "    Implementa a cabe√ßa da arquitetura Transformer dada pela equa√ß√£o \n",
    "    (softmax((q' @ k'.T) / sqrt(D_k)) @ v')' onde ' √© a opera√ß√£o de proje√ß√£o linear para cada uma dos \n",
    "    tensores no seu espa√ßo vetorial adequado. Novamente, as opera√ß√µes, conforme exerc√≠cio anterior, \n",
    "    podem ser utilizadas a partir de uma chamada de fun√ß√£o.\n",
    "    \n",
    "    Args:\n",
    "        q (torch.tensor): Tensor de query de tamanho (N, L, D)\n",
    "        k (torch.tensor): Tensor de key de tamanho (N, L, D)\n",
    "        v (torch.tensor): Tensor de value de tamanho (N, L, D)\n",
    "        proj_q (nn.Module): Opera√ß√£o que realiza a proje√ß√£o linear de um tensor (N, L, D) em \n",
    "            H espa√ßos vetoriais de dimens√£o D_k, resultando em um tensor (N, H, L, D_k)\n",
    "        proj_k (nn.Module): Opera√ß√£o que realiza a proje√ß√£o linear de um tensor (N, L, D) em\n",
    "             H espa√ßos vetoriais de dimens√£o D_k, resultando em um tensor (N, H, L, D_k)\n",
    "        proj_v (nn.Module): Opera√ß√£o que realiza a proje√ß√£o linear de um tensor (N, L, D) em\n",
    "             H espa√ßos vetoriais de dimens√£o D_v, resultando em um tensor (N, H, L, D_v)\n",
    "        reproj (nn.Module): Opera√ß√£o que realiza a concatena√ß√£o e proje√ß√£o linear de um tensor \n",
    "             (N, H, L, D_v) de dimens√£o E2, resultando em um tensor (N, L, D)\n",
    "        softmax (nn.Module): Opera√ß√£o de softmax sobre a √∫ltima dimens√£o do tensor.\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor: Tensor de tamanho (N, L, D) resultado final da opera√ß√£o do transformer sobre q, k, v\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return resultado\n",
    "\n",
    "class Projeta(nn.Linear):\n",
    "    def __init__(self, dim_in, quant_projs, dim_out, **kwargs):\n",
    "        super().__init__(dim_in, quant_projs * dim_out, **kwargs)\n",
    "        self.quant_projs = quant_projs\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_flat = super().forward(x)\n",
    "        return x_flat.reshape(*x_flat.shape[:2], self.quant_projs, -1).transpose(1, 2)\n",
    "\n",
    "class ReProjeta(nn.Linear):\n",
    "    def __init__(self, dim_in, quant_projs, dim_out, **kwargs):\n",
    "        super().__init__(dim_in * quant_projs, dim_out, **kwargs)\n",
    "        self.quant_projs = quant_projs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_flat = x.transpose(1, 2).contiguous()\n",
    "        return super().forward(x_flat.reshape(*x_flat.shape[:2], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a6b90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2570b4798e6cd75aae610a8b6a03e4df",
     "grade": true,
     "grade_id": "testa-transformer",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=-1)\n",
    "proj_q = Projeta(4, 2, 3)\n",
    "proj_k = Projeta(4, 2, 3)\n",
    "proj_v = Projeta(4, 2, 5)\n",
    "reproj = ReProjeta(5, 2, 4)\n",
    "proj_q.load_state_dict(OrderedDict([('weight', torch.tensor([[-0.2903,  0.1144, -0.2388,  0.3808],\n",
    "                      [ 0.4571, -0.1722, -0.3059,  0.0529],\n",
    "                      [-0.0764,  0.4670, -0.2218, -0.3888],\n",
    "                      [ 0.4820, -0.0895,  0.0496, -0.4707],\n",
    "                      [ 0.0030, -0.0348,  0.4132,  0.3539],\n",
    "                      [-0.2953,  0.2528,  0.2744, -0.0833]])),\n",
    "    ('bias', torch.tensor([-0.2345, -0.0744,  0.1075, -0.1458, -0.4157, -0.3114]))]))\n",
    "proj_k.load_state_dict(OrderedDict([('weight', torch.tensor([[ 0.2761, -0.4321,  0.3839,  0.1454],\n",
    "                      [-0.3980,  0.1969, -0.4166, -0.2317],\n",
    "                      [-0.1690, -0.1395,  0.3167, -0.3027],\n",
    "                      [-0.1422, -0.2583, -0.4430, -0.0448],\n",
    "                      [ 0.4761,  0.1354,  0.1436,  0.3219],\n",
    "                      [ 0.3956,  0.0885,  0.2519,  0.1227]])),\n",
    "    ('bias', torch.tensor([0.4507, 0.3675, 0.3032, 0.3873, 0.4495, 0.1289]))]))\n",
    "proj_v.load_state_dict(OrderedDict([('weight', torch.tensor([[ 0.2719, -0.0664,  0.3742,  0.3409],\n",
    "                      [ 0.1177,  0.4588, -0.3498, -0.4507],\n",
    "                      [-0.4851, -0.3594,  0.2124, -0.1817],\n",
    "                      [-0.3162, -0.1503, -0.1955,  0.3816],\n",
    "                      [ 0.0005, -0.0776, -0.4964, -0.1608],\n",
    "                      [ 0.0581, -0.1783, -0.2951,  0.0964],\n",
    "                      [-0.3771,  0.1194, -0.4692,  0.1051],\n",
    "                      [-0.4773, -0.0826,  0.4722, -0.2247],\n",
    "                      [-0.1419,  0.0064,  0.3859,  0.1678],\n",
    "                      [ 0.2845, -0.4944,  0.4023, -0.2722]])),\n",
    "('bias', torch.tensor([ 0.1750,  0.1422, -0.3162,  0.2938,  0.0050, -0.0249,  0.2706, -0.2545, 0.0081, -0.2179]))]))\n",
    "reproj.load_state_dict(OrderedDict([('weight',\n",
    "    torch.tensor([[ 0.2090, -0.1364,  0.2534,  0.0466, -0.1310,  0.1216,  0.0299, -0.2586, 0.3095, -0.2708],\n",
    "            [-0.2939,  0.2829, -0.0410,  0.2643,  0.0008, -0.3008,  0.2150,  0.0737, -0.0611, -0.0701],\n",
    "            [ 0.2135, -0.1298, -0.3017,  0.2684, -0.0917, -0.1428,  0.1363, -0.1012, 0.2472,  0.2877],\n",
    "            [ 0.1128,  0.0359, -0.1215,  0.2214,  0.2173, -0.1789,  0.1038,  0.0059, 0.2911, -0.0398]])),\n",
    "             ('bias', torch.tensor([ 0.2898,  0.0237, -0.2518, -0.2610]))]))\n",
    "sequencia = torch.tensor([[\n",
    "                   [ 0.1177,  0.4588, -0.3498, -0.4507],\n",
    "                   [-0.0764,  0.4670, -0.2218, -0.3888],\n",
    "                   [-0.3771,  0.1194, -0.4692,  0.1051],\n",
    "                   [-0.1419,  0.0064,  0.3859,  0.1678],\n",
    "                   [-0.3162, -0.1503, -0.1955,  0.3816],\n",
    "                   ]])\n",
    "assert torch.norm(cabeca_transformer(sequencia, sequencia, sequencia, proj_q, proj_k, proj_v, reproj, softmax) -\\\n",
    "    torch.tensor([[[ 0.3581,  0.3061, -0.1407, -0.0638],\n",
    "                   [ 0.3595,  0.3037, -0.1386, -0.0639],\n",
    "                   [ 0.3618,  0.3005, -0.1352, -0.0635],\n",
    "                   [ 0.3583,  0.3030, -0.1361, -0.0625],\n",
    "                   [ 0.3604,  0.3014, -0.1349, -0.0626]]])).item() < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0fc79",
   "metadata": {},
   "source": [
    "## Modelo de Difus√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b17d7b",
   "metadata": {},
   "source": [
    "![Exemplos Gerados](https://techcrunch.com/wp-content/uploads/2022/08/53118410-9cce-468a-8bf6-1b8ce4dd1390_1600x925.webp?resize=1200,694)\n",
    "\n",
    "Os modelos j√° baixados no drive foram distribu√≠dos pelo [HuggingFace](https://huggingface.co/) e o exemplo abaixo pelo [FastAI](https://github.com/fastai/diffusion-nbs/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniza_codifica_string(lista_strings, tokenizer, text_encoder):\n",
    "    ml = tokenizer.model_max_length\n",
    "    inp = tokenizer(lista_strings, padding=\"max_length\", max_length=ml, truncation=True, return_tensors=\"pt\")\n",
    "    codificado = text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n",
    "    vaz = tokenizer([\"\"] * len(lista_strings), padding=\"max_length\", max_length=ml, truncation=True, return_tensors=\"pt\")\n",
    "    vazio = text_encoder(vaz.input_ids.to(\"cuda\"))[0].half()\n",
    "    return torch.cat([vazio, codificado])\n",
    "\n",
    "def mostra_imagem(espaco_latente, vae, indice=0):\n",
    "    with torch.no_grad():\n",
    "        imagem_normalizada = vae.decode(1 / 0.18215 * espaco_latente).sample[indice]\n",
    "    image = (imagem_normalizada/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n",
    "    return Image.fromarray((image*255).round().astype(\"uint8\"))\n",
    "\n",
    "def loop_difusao(texto_codificado, unet, escalonador, steps=70, g=7.5, width=512, height=512, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    escalonador.set_timesteps(steps)\n",
    "    espaco_latente = torch.randn((texto_codificado.shape[0]//2, unet.in_channels, height//8, width//8))\n",
    "    espaco_latente = espaco_latente.to(\"cuda\").half() * escalonador.init_noise_sigma\n",
    "    for step in tqdm(escalonador.timesteps):\n",
    "        inp = escalonador.scale_model_input(torch.cat([espaco_latente] * 2), step)\n",
    "        with torch.no_grad():\n",
    "            u, t = unet(inp, step, encoder_hidden_states=texto_codificado).sample.chunk(2)\n",
    "        ruido_estimado = u + g*(t-u)\n",
    "        espaco_latente = escalonador.step(ruido_estimado, step, espaco_latente).prev_sample\n",
    "    return espaco_latente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca21362",
   "metadata": {},
   "source": [
    "Observe a implementa√ß√£o das fun√ß√µes acima, que realizam a tokeniza√ß√£o e a codifica√ß√£o das entradas textuais, a convers√£o do espa√ßo latente para uma imagem em pixels RGB e, principalmente, o loop de difus√£o, no qual o espa√ßo latente √© iterativamente atualizado, diminuindo-se o ru√≠do, que √© estimado pelo modelo a cada itera√ß√£o.\n",
    "\n",
    "Investigue o modelo do Stable Diffusion gerando a sua pr√≥pria string para guiar a difus√£o. Sugiro fortemente que seja em ingl√™s, que √© a principal linguagem que o codificador de texto (CLIP) foi treinado. (1 ponto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31376d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "445484a4609549d5a3256c5690383c0d",
     "grade": false,
     "grade_id": "difusao",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gerar_entrada_textual():\n",
    "    \"\"\"\n",
    "    Retorna uma string de entrada parar o modelo de Difus√£o\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    Returns:\n",
    "        str: String para guiar a difus√£o\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return string_entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e1455",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "211c9776e20e574ab9e2cd70a37378ee",
     "grade": true,
     "grade_id": "testa-difusao",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "string_entrada = gerar_entrada_textual()\n",
    "assert string_entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88375f",
   "metadata": {},
   "source": [
    "E agora veja a execu√ß√£o do Stable Diffusion com a sua entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71685dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert executar_transformer # Esse assert n√£o ser√° corrigido\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", use_auth_token=hf_token, torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", use_auth_token=hf_token, torch_dtype=torch.float16).to(\"cuda\")\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", use_auth_token=hf_token, torch_dtype=torch.float16).to(\"cuda\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=hf_token, subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert executar_transformer # Esse assert n√£o ser√° corrigido\n",
    "escalonador = LMSDiscreteScheduler(beta_start=0.00085,beta_end=0.012,beta_schedule=\"scaled_linear\",num_train_timesteps=1000)\n",
    "texto_codificado = tokeniza_codifica_string([string_entrada], tokenizer, text_encoder)\n",
    "imagens_geradas = loop_difusao(texto_codificado, unet, escalonador, steps=70, seed=42)\n",
    "mostra_imagem(imagens_geradas, vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca032da",
   "metadata": {},
   "source": [
    "Implemente o loop abaixo de difus√£o com os coeficientes de atualiza√ß√£o simples, no qual a imagem original √© recuperada pela subtra√ß√£o do seu respectivo ru√≠do, ambos ponderados por fatores. (2 pontos)\n",
    "\n",
    "Por ser um modelo mais simples de escalonamento, o DDPM (Denoising diffusion probabilistic models) precisa de mais passos de difus√£o, cada um com menor amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe29ed",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0cfeee9f91a63b3e7a04d880e64d070",
     "grade": false,
     "grade_id": "escalonador-simples",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def atualizacao_simples(ruido_estimado, imagem_latente, fator_ruido, fator_imagem):\n",
    "    \"\"\"\n",
    "    Implementa uma simples atualiza√ß√£o do escalonador de forma a remover o ru√≠do presente na imagem latente.\n",
    "    O ru√≠do √© multiplicado pelo fator beta e a imagem pelo fator alfa, que dependem do passo de difus√£o.\n",
    "    \n",
    "    Args:\n",
    "        ruido_estimado (torch.tensor): Tensor de ru√≠do no espa√ßo latente de tamanho (N, C, H, W)\n",
    "        imagem_latente (torch.tensor): Tensor de imagem no espa√ßo latente de tamanho (N, C, H, W)\n",
    "        fator_ruido (float): Fator que deve multiplicar o ru√≠do\n",
    "        fator_imagem (float): Fator que deve multiplicar a imagem latente\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor: Tensor de tamanho (N, C, H, W) estimado a partir da remo√ß√£o do ru√≠do da imagem, poderados\n",
    "            por seus respectivos fatores.\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return imagem_latente_reconstruida\n",
    "\n",
    "def loop_difusao_simples(texto_codificado, unet, steps=70, g=7.5, width=512, height=512, seed=42, train_steps=1000, beta_start=0.00085, beta_end=0.012):\n",
    "    torch.manual_seed(seed)\n",
    "    betas = np.linspace(beta_start, beta_end, train_steps)\n",
    "    alfas = 1.0 - betas\n",
    "    alfa_prod = np.pad(np.cumprod(alfas, axis=0), (0, 1), constant_values=1)\n",
    "    beta_prod = 1 - alfa_prod\n",
    "    espaco_latente = torch.randn((texto_codificado.shape[0]//2, unet.in_channels, height//8, width//8)).to(\"cuda\").half()\n",
    "    for t in tqdm(np.arange(0, train_steps, train_steps // steps)[::-1]):\n",
    "        inp = torch.cat([espaco_latente] * 2)\n",
    "        with torch.no_grad():\n",
    "            uncond, text = unet(inp, t, encoder_hidden_states=texto_codificado).sample.chunk(2)\n",
    "        ruido_estimado = uncond + g*(text-uncond)\n",
    "        espaco_latente_original = torch.clamp(atualizacao_simples(ruido_estimado, espaco_latente, \n",
    "                                beta_prod[t]**0.5/alfa_prod[t]**0.5, 1/alfa_prod[t]**0.5), -1, 1)\n",
    "        coeff_original = alfa_prod[t-1]**0.5 * betas[t]/beta_prod[t]\n",
    "        coeff_atual = alfas[t]**0.5 * beta_prod[t-1]/beta_prod[t]\n",
    "        espaco_latente = coeff_original*espaco_latente_original + coeff_atual*espaco_latente\n",
    "        variancia = np.clip(beta_prod[t-1]/beta_prod[t] * betas[t], 1e-20, None)\n",
    "        ruido = torch.randn(ruido_estimado.shape, device=ruido_estimado.device, dtype=ruido_estimado.dtype)\n",
    "        espaco_latente = espaco_latente + variancia**0.5 * ruido\n",
    "    return espaco_latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdfcf0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb953bd38e7326b1f6d833665860c822",
     "grade": true,
     "grade_id": "testa-escalonador-simples",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert torch.norm(atualizacao_simples(torch.tensor([[[[0.1, -0.1], [0.2,-0.2]]]]), \n",
    "                                      torch.tensor([[[[2, 3], [4,3]]]]), 0.4, 0.9) - \\\n",
    "                  torch.tensor([[[[1.76, 2.74],[3.52, 2.78]]]])) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5b804",
   "metadata": {},
   "source": [
    "Apesar do nome de simples, e de ser a mais simples que realmente √© empregada durante o treino do modelo, a DDPM requer uma quantidade de itera√ß√µes que justamente de aproxima do treino, no caso, mil passos, para que tenha uma converg√™ncia adequada. Por isso o outro m√©todo acaba sendo mais de uma ordem de grandeza mais r√°pido para infer√™ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert executar_transformer # Esse assert n√£o ser√° corrigido\n",
    "imagens_geradas = loop_difusao_simples(texto_codificado, unet, steps=1000, seed=42)\n",
    "mostra_imagem(imagens_geradas, vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159d3e3",
   "metadata": {},
   "source": [
    "Sinta-se a vontade para explorar o modelo e outros tipos de entradas textuais."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
