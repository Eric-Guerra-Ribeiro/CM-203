{"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"f1e1726ec4c594474ee76ab224186eb1","grade":false,"grade_id":"header","locked":true,"schema_version":3,"solution":false},"id":"CopV9pO0cuXq"},"source":["**Instituto Tecnol√≥gico de Aeron√°utica ‚Äì ITA**\n","\n","**Vis√£o Computacional - CM-203**\n","\n","**Professores:** \n","\n","Elcio Hideiti Shiguemori\n","\n","Gabriel Adriano de Melo\n","\n","Marcos Ricardo Omena de Albuquerque Maximo\n","\n","# Laborat√≥rio 1 ‚Äì Calibra√ß√£o de C√¢mera\n"]},{"cell_type":"markdown","metadata":{"id":"NiZJvgDBcuXz"},"source":["Observa√ß√£o: se estiver usando Colab, para alterar este notebook, salve uma c√≥pia no seu Drive usando ``File > Save a copy in Drive``."]},{"cell_type":"markdown","metadata":{"id":"IWkJABBAcuX0"},"source":["Execute a c√©lula a seguir para instalar as depend√™ncias."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbnUuvIhcuX0"},"outputs":[],"source":["!pip3 install opencv-contrib-python==4.6.0.66 Pillow==7.1.2 matplotlib==3.2.2 scipy==1.7.3"]},{"cell_type":"markdown","metadata":{"id":"1_6NggvkcuX1"},"source":["Execute a c√©lula a seguir para importar as bibliotecas necess√°rias."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_fS5esEcuX2"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from scipy.spatial.transform import Rotation\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","from glob import glob\n","from pathlib import Path"]},{"cell_type":"markdown","metadata":{"id":"AEGpayzdcuX2"},"source":["Execute a c√©lula a seguir para baixar as imagens de calibra√ß√£o da c√¢mera."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnF9wW3hcuX3"},"outputs":[],"source":["import zipfile\n","import os\n","\n","# Downloading the zip file\n","!gdown --id 1vg2fnoLjcYAdF44HxwK41basZUj0xYMN\n","\n","with zipfile.ZipFile(\"calib_esq.zip\",\"r\") as zip_ref:\n","    zip_ref.extractall() # Extracts in current directory\n","\n","# Since we have already unzipped the files, we may now delete the zip file\n","os.remove(\"calib_esq.zip\")\n","\n","# Defining the path where the images are located\n","images_path = 'calib_esq/'"]},{"cell_type":"markdown","metadata":{"id":"1YMFQJdpcuX4"},"source":["## Introdu√ß√£o sobre Imagens\n","\n","Vamos carregar uma imagem para explorar como imagens s√£o armazenadas. Vamos carregar uma imagem de calibra√ß√£o com `cv2.imread()` e transform√°-las em escala de cinza com `cv2.cvtColor()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UoE6H_VxcuX4"},"outputs":[],"source":["image = cv2.imread(images_path + '00.png', cv2.IMREAD_ANYDEPTH) # Loads the image\n","plt.imshow(image, cmap='gray', vmin=0, vmax=255) # Shows the image in grayscale\n","plt.axis(False) # Removes the axes for better visualization\n","print(image.shape) # Prints the dimensions of the matrix that represents the image"]},{"cell_type":"markdown","metadata":{"id":"AjAQUF4GcuX5"},"source":["A c√¢mera industrial em quest√£o entrega a imagem no formato bruto (*raw*), sem nenhum processamento. Como ela √© colorida, ela tem um padr√£o de BAYER. No caso espec√≠fico dela o padr√£o √© Green-Blue (GB), como mostrada na figura abaixo.\n","![1st Vision GB Pattern](https://www.1stvision.com/machine-vision-solutions/wp-content/uploads/2019/12/RGB-Filter-1.jpg)\n","Assim, usa-se a fun√ß√£o `cv2.cvtColor()` para fazer a interpola√ß√£o linear para cada canal de cor. O segundo argumento dela √© o enum que estabelece o tipo de convers√£o de cor. Vamos converter do padr√£o `BAYER_GB` para (2 *to*) `RGB`.\n","\n","**Observa√ß√£o:** uma grande fonte de confus√£o √© que a OpenCV trabalha `BGR`, por√©m a conven√ß√£o `RGB` √© muito mais comum. Inclusive, as demais bibliotecas que usamos neste laborat√≥rio (Matplotlib e Pillow) usam `RGB`. Cuidado com essa invers√£o."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywuQLmaRcuX5"},"outputs":[],"source":["image_color = cv2.cvtColor(image, cv2.COLOR_BAYER_GB2RGB) # Debayering\n","plt.imshow(image_color)\n","plt.axis(False)\n","print(image_color.shape)"]},{"cell_type":"markdown","metadata":{"id":"esGzZuTpcuX6"},"source":["Perceba que as imagens s√£o simplesmente matrizes. No caso da imagem bruta, ou em escala de cinza, a dimens√£o da matriz √© simplesmente `(altura, largura)`. J√° na imagem colorida, tem-se `(altura, largura, canal_de_cor)`. H√° outras bibliotecas que colocam o canal de cor na primeira dimens√£o (*channels first*), em vez da  (*channels last*), mas o mais comum √© colocar os canais de cor na √∫ltima.\n","\n","**Observa√ß√£o:** perceba que pela conven√ß√£o de armazenamento `(linha, coluna)` de uma matriz, os √≠ndices acabam ficando trocados para os eixos $x$ e $y$. Tenha muito cuidado para n√£o fazer confus√£o.\n","\n","No nosso c√≥digo, as matrizes s√£o abstra√≠das por um objeto NumPy `ndarray`. Dessa forma, a sua manipula√ß√£o e indexa√ß√£o √© igual a de uma matriz num√©rica comum."]},{"cell_type":"markdown","metadata":{"id":"tqaIzVdAcuX6"},"source":["Implemente a fun√ß√£o ``load_images_and_debayer()``, que carrega todas as imagens .png de uma determinada pasta, realiza o debayering delas atrav√©s da OpenCV (assumindo formato BAYER GB) e retorna uma lista de imagens coloridas no formato BGR."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"70b5b922994684a6a458edc8b4f67c2f","grade":false,"grade_id":"load_images_and_debayer_answer","locked":false,"schema_version":3,"solution":true,"task":false},"id":"CSyL1GAccuX7"},"outputs":[],"source":["def load_images_and_debayer(path: str):\n","    \"\"\"\n","    Receives a path with .png images in BAYER BG format and returns a list of imagens in BGR format.\n","    :param path: path where the BAYER BG images are located.\n","    :return: list with BGR images.\n","    \"\"\"\n","    images = []\n","    for image_file in sorted(glob(path + '/*.png')):\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","    return images"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"fc8c227efccb80b7bca1200d70dad401","grade":true,"grade_id":"load_images_and_debayer_tests","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"sBWvHOg4cuX7"},"outputs":[],"source":["images = load_images_and_debayer(images_path)\n","assert len(images) == 35\n","assert images[0].shape == (1080, 1920, 3)\n","assert np.all(images[19][19, 19] == [0, 6, 4])\n","assert np.all(images[19][400, 520] == [18, 33, 47])\n","assert np.all(images[12][12, 12] == [0, 0, 7])"]},{"cell_type":"markdown","metadata":{"id":"NNitTO6gcuX8"},"source":["A c√©lula abaixo exibe todas as imagens de calibra√ß√£o."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lEXUJpkacuX8"},"outputs":[],"source":["# Loads the images and debayer them\n","images = load_images_and_debayer(images_path)\n","\n","num_images = len(images)\n","columns = 4\n","\n","# Shows all the calibration images in a grid\n","plt.figure(figsize=(60, 80))\n","for i, image in enumerate(images):\n","    plt.subplot(int(len(images) / columns) + 1, columns, i + 1)\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    plt.imshow(image_rgb)\n","    plt.axis(False)"]},{"cell_type":"markdown","metadata":{"id":"eizLxS6lcuX8"},"source":["## Calibra√ß√£o de Par√¢metros Intr√≠secos e de Distor√ß√µes\n","\n","Para as imagens de calibra√ß√£o, √© importante ter uma distribui√ß√£o dos pontos de maneira uniforme ao longo de todo o campo de vis√£o, principalmente pertos dos cantos, onde ocorrem distor√ß√µes maiores.\n","\n","Outro ponto importante √© variar a inclina√ß√£o do plano de calibra√ß√£o com rela√ß√£o ao plano da c√¢mera, para que eles n√£o sejam paralelo.\n","\n","Para a estima√ß√£o dos par√¢metros, quanto maior a quantidade de v√©rtices do tabuleiro, mais est√°vel √© a estima√ß√£o. √â claro que um tabuleiro muito grande fica com v√©rtices muito pr√≥ximos e se estiver longe ou se a resolu√ß√£o for pequena n√£o ser√° poss√≠vel identificar corretamente os v√©rtices.\n","\n","No tabuleiro de xadrez das imagens, usou-se 10 e 7 v√©rtices internos em cada dimens√£o, totalizando 70 v√©rtices. O tabuleiro foi impresso em papel A0 com cada lado medindo 0,1 m."]},{"cell_type":"markdown","metadata":{"id":"2T2f-IbScuX8"},"source":["Depois de termos carregados as imagens, o primeiro passo √© fazer de cada v√©rtice/canto (*corner*) interno do tabuleiro. Para isso, usa-se a fun√ß√£o `cv2.findChessboardCorners()`, cujo funcionamento interno veremos nas pr√≥ximas aulas. A ideia √© que ela retorna as coordenadas 2D (na imagem) dos v√©rtices internos ordenados. Ela usa opera√ß√µes de *thresholding* e por isso aceita apenas uma imagem com um canal. Outro par√¢metro importante √© a quantidade de v√©rtices internos do tabuleiro."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zh1_GhbWcuX8"},"outputs":[],"source":["image = cv2.imread(images_path + '05.png', cv2.IMREAD_ANYDEPTH)\n","num_vertices = (10, 7)\n","image_grayscale = cv2.cvtColor(image, cv2.COLOR_BAYER_GB2GRAY)\n","ret, vertices = cv2.findChessboardCorners(image_grayscale, num_vertices)\n","print(vertices.shape)"]},{"cell_type":"markdown","metadata":{"id":"PMlB-UhFcuX9"},"source":["Note o formato da matriz de v√©rtices retornado pelo OpenCV. A primeira dimens√£o se refere ao ponto, a segunda √© apenas unit√°ria e a terceira armazena as coordenadas $x$ e $y$ do ponto. N√≥s podemos visualizar exatamente a ordem e os v√©rtices internos que s√£o retornados, atrav√©s da fun√ß√£o `cv2.drawChessboardCorners()`. S√≥ atentar que ela desenha sobre a imagem original e tamb√©m com o padr√£o de cores do OpenCV."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fwkqm_ahcuX9"},"outputs":[],"source":["image_color = cv2.cvtColor(image, cv2.COLOR_BAYER_GB2BGR)\n","image_corners = cv2.drawChessboardCorners(image_color.copy(), num_vertices, vertices, ret)\n","Image.fromarray(cv2.cvtColor(image_corners, cv2.COLOR_BGR2RGB))"]},{"cell_type":"markdown","metadata":{"id":"qEnDK0WrcuX-"},"source":["Perceba que ele come√ßa a percorrer pela primeira dimens√£o do `num_vertices`, no caso a maior, que est√° na horizontal. Depois, mude essa par√¢metro para (7, 10) e veja a diferen√ßa.\n","\n","As coordenadas do tabuleiro de xadrez da fun√ß√£o do `cv2.findChessboardCorners()` tem um n√≠vel de exatid√£o da ordem de 1 pixel, que √© a discretiza√ß√£o de localiza√ß√£o para uma imagem bin√°ria (a binariza√ß√£o da imagem, que significa transformar os valores entre 0 a 255 para apenas 0 ou 255 faz parte do *pipeline*). Mas existem m√©todos que se aproveitam do gradiente da imagem, isto √©, do fato de que a borda de alto contraste passa pelo v√©rtice, para interpolar uma posi√ß√£o a n√≠vel de subpixel. Para isso tem-se a fun√ß√£o `cv2.cornerSubPix()`, que j√° parte de uma posi√ß√£o inicial da proposi√ß√£o do v√©rtice para iterativamente refin√°-la calculando os gradientes da imagem em torno de uma regi√£o.\n","\n","Escreva uma fun√ß√£o que receba uma lista de imagens BGR e retorna apenas uma lista de posi√ß√µes dos v√©rtices internos do tabuleiro de xadrez encontrado em cada imagem, j√° refinados a n√≠vel de subpixel.\n","\n","Para ajudar, veja (üêí):\n","* Documenta√ß√£o da fun√ß√£o ``cv2.cornerSubPix()``: https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga354e0d7c86d0d9da75de9b9701a9a87e\n","* Tutorial de calibra√ß√£o de c√¢mera: https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"2d99acd830d58dd0a801e5e7dca63606","grade":false,"grade_id":"find_and_refine_chessboard_corners_answer","locked":false,"schema_version":3,"solution":true,"task":false},"id":"GdFiiKzocuX-"},"outputs":[],"source":["def find_and_refine_chessboard_corners(images_list, num_vertices, win_size, criteria):\n","    \"\"\"\n","    Receives a list of BGR images and returns detected corners of a chessboard calibration pattern\n","    after subpixel refinement. Uses (-1, -1) for the zero zone.\n","    :param images_list: list of BGR images.\n","    :param num_vertices: tuple containing the number of vertices along each dimension of the chessboard.\n","    :param win_size: refinement window size.\n","    :param criteria: termination criteria for the corner refinement.\n","    :return: list containing the corners found for each image. Each element of this list is a array\n","             containing the vertices found in the given image.\n","    \"\"\"\n","    vertices_list = []\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    return vertices_list"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"30fccc68fce5b7351e89876e1d8afb4a","grade":true,"grade_id":"find_and_refine_chessboard_corners_tests","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"5Gx5FuZbcuX-"},"outputs":[],"source":["criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 50, 5e-4)\n","vertices_list = find_and_refine_chessboard_corners(images, num_vertices, (9, 9), criteria)\n","assert len(vertices_list) < len(images)\n","assert vertices_list[0].shape == (num_vertices[0] * num_vertices[1], 1, 2)\n","assert np.linalg.norm(vertices_list[0][0, 0, :] - [1041.6,  785.1]) < 0.3"]},{"cell_type":"markdown","metadata":{"id":"9WKtN2S7cuX-"},"source":["Agora que j√° temos as coordenadas 2D de cada posi√ß√£o do tabuleiro, n√≥s temos que saber as coordenadas 3D do tabuleiro, para podermos determinar as matrizes intr√≠nseca e extr√≠nseca, e os par√¢metros da distor√ß√£o.\n","\n","Se n√≥s soubessemos a posi√ß√£o 3D de cada ponto do tabuleiro com rela√ß√£o √† c√¢mera, j√° poder√≠amos otimizar direto a matriz intr√≠nseca e os par√¢metros de distor√ß√£o, mas como n√£o sabemos, temos que estimar essa posi√ß√£o do tabuleiro. Mas n√≥s sabemos a posi√ß√£o de um v√©rtice com rela√ß√£o aos outros. Assim podemos definir um sistema de coordenadas do objeto (considerado um corpo r√≠gido), no qual conhecemos exatamente a coordenada de cada v√©rtice, e descobrir apenas a transla√ß√£o e rota√ß√£o (postura 6D - matriz extr√≠nseca) que leva do sistema de refer√™ncia do objeto para o sistema de refer√™ncia da c√¢mera.\n","\n","Vamos adotar a seguinte conven√ß√£o para os v√©rtices do tabuleiro (qualquer outra poderia ser adotada, desde que seja consistente): o primeiro v√©rtice do canto superior esquerdo √© a origem desse sistema de coordenadas, o eixo X √© crescente para a direita e o eixo Y crescente para baixo. O eixo Z √© ortogonal ao plano, seguindo a conve√ß√£o da regra da m√£o direita.\n","\n","Assim, implemente uma fun√ß√£o que retorne as coordenadas 3D de cada v√©rtice (na mesma ordem definida pela OpenCV, i.e. come√ßando pela primeira dimens√£o) no sistema de coordenadas do tabuleiro (com a conven√ß√£o definida anteriormente), em metros."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"879321afe3caad624bd5fabef5f23a92","grade":false,"grade_id":"get_board_vertices_answer","locked":false,"schema_version":3,"solution":true,"task":false},"id":"OiUG5ps_cuX_"},"outputs":[],"source":["def get_board_vertices(num_vertices, square_side):\n","    \"\"\"\n","    Returns a matrix with 3D points of each ordered vertice, in meters.\n","    :param num_vertices: tuple defining the amount of vertices in each dimension, following OpenCV's convention.\n","    :param square_side: the length of the side of each square.\n","    :return: \n","    \"\"\"\n","    vertices3d = np.zeros((num_vertices[0] * num_vertices[1], 3), dtype=np.float32)\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    return vertices3d"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"cd9ad8f561826c407eba137e067cfbfa","grade":true,"grade_id":"get_board_vertices_tests","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"LHKgFcUUcuX_"},"outputs":[],"source":["assert np.all(get_board_vertices((2, 2), 1) == \n","                            np.array([[0., 0., 0.],\n","                                      [1., 0., 0.],\n","                                      [0., 1., 0.],\n","                                      [1., 1., 0.]], dtype=np.float32))\n","vertices3d = get_board_vertices((10, 7), 0.10)"]},{"cell_type":"markdown","metadata":{"id":"8z3Yfe5jcuX_"},"source":["A ideia √© que com as coordenadas 3D de cada v√©rtice, descritas no sistema de refer√™ncia do tabuleiro (corpo r√≠gido), n√≥s possamos encontrar na calibra√ß√£o a mudan√ßa de base do tabuleiro para c√¢mera, isto √©, a postura.\n","\n","![Rota√ß√£o e Transla√ß√£o](https://www.researchgate.net/profile/Rushikesh-Amrutsamanvar/publication/337558917/figure/fig3/AS:830939248734214@1575122763812/Rotation-and-translation-parameters-of-the-camera_W640.jpg)\n","\n","Assim, basta encontrarmos a rota√ß√£o e a transla√ß√£o que nos leva do sistema de coordenadas do corpo para o sistema de coordenadas da c√¢mera. Assim n√≥s expressamos a mudan√ßa pela seguinte express√£o: $X' = R X + T$. √â comum tamb√©m concatenar horizontalmente (`np.hstack`) as matrizes de rota√ß√£o e transla√ß√£o.\n","\n","Observe o exemplo abaixo de uma matriz com rota√ß√µes de 45¬∫ no eixo X e depois no eixo Y (√¢ngulos de Euler intr√≠nsecos) e uma transla√ß√£o de 9 metros. N√£o confunda, primeiro a rota√ß√£o e depois a transla√ß√£o, fica errado se voc√™ alterar a ordem das opera√ß√µes (pois sen√£o a transla√ß√£o seria em outro eixo que n√£o o da c√¢mera)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCkNWnfbcuX_"},"outputs":[],"source":["rotation_matrix = Rotation.from_euler('XYZ', [45.0, 45.0, 0.0], degrees=True).as_matrix()\n","translation_vector = np.array([0.0, 0.0, 9.0]).reshape(-1, 1)\n","(rotation_matrix @ vertices3d.T + translation_vector).T"]},{"cell_type":"markdown","metadata":{"id":"C4_at1xvcuYA"},"source":["Finalmente, com a rela√ß√£o de pontos 2D e os seus respectivos pontos 3D, no sistema de coordenadas de um corpo r√≠gido, podemos descobrir o mapeamento entre 2D e 3D, segundo o nosso modelo de geometria projetiva, otimizando os par√¢metros da c√¢mera e da postura do objetivo simultaneamente.\n","\n","Para isso podemos usar a fun√ß√£o `cv2.calibrateCameraExtended()`, que faz essa otimiza√ß√£o iterativamente. Al√©m dos par√¢metros otimizados, nos retorna as incertezas e os erros residuais da otimiza√ß√£o. Esses erros vem da reproje√ß√£o dos pontos 3D, segundo a postura encontrada e os par√¢metros da c√¢mera, e calculando a dist√¢ncia do ponto reprojetada para o ponto 2D determinado.\n","\n","Implemente `calibrate_camera()` abaixo, usando a fun√ß√£o da OpenCV para calibrar. Para descobrir como utilizar `cv2.calibrateCameraExtended()`, verifique a documenta√ß√£o e *tutoriais* da OpenCV. Procurar a documenta√ß√£o desta fun√ß√£o pode ser vista aqui: https://docs.opencv.org/4.6.0/d9/d0c/group__calib3d.html\n","\n","Aten√ß√£o que o par√¢metro `objectPoints` da fun√ß√£o √© uma lista de `coords3d` cujo cada elemento est√° associado ao seu respectivo par 2D, que √© uma lista onde cada elemento representa um tabuleiro (`imagePoints`). No caso, como todas as fotos s√£o do mesmo tabuleiro do mesmo tamanho, √© s√≥ criar uma lista com $N$ elementos repetidos de `verts3d`.\n","\n","Atente que o `imageSize` √© o tamanho da imagem em largura por altura (que √© o contr√°rio do shape de linhas por colunas).\n","\n","Os par√¢metros de `cameraMatrix` e `distCoeffs` podem ser passados como `None`, na realidade eles s√£o um resqu√≠cio do c√≥digo em C que s√£o passados por refer√™ncia.\n","\n","As distor√ß√µes da lentes s√£o um mapeamento 2D-2D, biun√≠voco, cont√≠nuo e suave. No modelo mais simples de 5 par√¢metros, tem uma componente radial (a partir do centro √≥ptico $c_x$ e $c_y$) e outra cruzada (tangencial).\n","![Distor√ß√µes 2d](https://docs.opencv.org/4.x/distortion_examples.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"134f32a3b089457be019f29bbcee4940","grade":false,"grade_id":"calibrate_camera_answer","locked":false,"schema_version":3,"solution":true,"task":false},"id":"fQNgmSjycuYA"},"outputs":[],"source":["def calibrate_camera(coords2d_list, coords3d, image_size, flags=None):\n","    \"\"\"\n","    Calibrates a camera using cv2.calibrateCameraExtended().\n","    :param coords2d_list: list of 2D coordinates of the board's vertices.\n","    :param coords3d: 3D coordinates of the board's vertices.\n","    :param image_size: the image size in terms of width and height.\n","    Returns the return values of cv2.calibrateCameraExtended(). Please, see OpenCV documentation.\n","    \"\"\"\n","    # Hint: coords3d represents the same object (the board) for all images. However, OpenCV expects\n","    # a list of coords3d as objectPoints. Therefore, you need to replicate coords3d using the number\n","    # of images.\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    return error, camera_matrix, dist_coeffs, rvecs, tvecs, std_intrinsics, std_extrinsics, per_view_errors"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"2fc553f0d9f160d3ed2711f28557eeda","grade":true,"grade_id":"calibrate_camera_tests","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"z0zpAC_BcuYA"},"outputs":[],"source":["error, camera_matrix, dist_coeffs, rvecs, tvecs, std_intrinsics, std_extrinsics, per_view_errors =\\\n","calibrate_camera(vertices_list[:-6], vertices3d, image_grayscale.shape[::-1], flags=None)\n","assert error < 1\n","assert np.linalg.norm(camera_matrix -\n","np.array([[1.27609629e+03, 0.00000000e+00, 9.48102766e+02],\n","          [0.00000000e+00, 1.28017819e+03, 5.27661311e+02],\n","          [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])) < 10\n","assert np.linalg.norm(dist_coeffs) < 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIugbR4FcuYA"},"outputs":[],"source":["error, camera_matrix, dist_coeffs, rvecs, tvecs, std_intrinsics, std_extrinsics, per_view_errors =\\\n","calibrate_camera(vertices_list[:], vertices3d, image_grayscale.shape[::-1], flags=cv2.CALIB_FIX_ASPECT_RATIO)\n","print('error: %f' % error)\n","print('per_view_errors:')\n","print(per_view_errors)\n","print('camera_matrix:')\n","print(camera_matrix)\n","print('dist_coeffs:')\n","print(dist_coeffs)\n","print('std_intrinsics:')\n","print(std_intrinsics)"]},{"cell_type":"markdown","metadata":{"id":"ZdzCIgiBcuYA"},"source":["Veja os resultados e reflita se fazem sentido:\n","o `error` e o `per_view_errors` s√£o erros de reproje√ß√£o em pixels;\n","Temos a matrix intr√≠seca `camera_matrix`, coeficiente de distor√ß√£o `dist_coeffs`, e o desvio padr√£o estimado para esses par√¢metros `std_intrinsics`."]},{"cell_type":"markdown","metadata":{"id":"1psAuncDcuYB"},"source":["A rota√ß√£o calculada pelo OpenCV `rvecs` est√° no formato de uma lista de Vetor de Rodrigues, em radianos. O Vetor de Rodrigues √© simplesmente um vetor de rota√ß√£o cuja dire√ß√£o define o eixo de rota√ß√£o e cuja magnitude define o √¢ngulo rotacionado em torno daquele eixo, com sentido dado pela regra da m√£o direita.\n","\n","![Vetor de Rodrigues](https://upload.wikimedia.org/wikipedia/commons/7/7b/Angle_axis_vector.svg)\n","\n","Por exemplo, um vetor de rodrigues `(0, 0, pi/4)` define uma rota√ß√£o de 45¬∫ em torno do eixo Z, que resulta na matriz de rota√ß√£o abaixo:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4isr-EndcuYB"},"outputs":[],"source":["Rotation.from_rotvec([0, 0, np.pi / 4]).as_matrix()"]},{"cell_type":"markdown","metadata":{"id":"hHeQ0o54cuYB"},"source":["A distor√ß√£o √© bem pequena para esse conjunto de lentes, sendo da ordem de alguns pixels nas bordas. Observe a regi√£o preta das boradas da imagem abaixo, depois que a distor√ß√£o foi removida. Foi adicionado um valor constante (*offset*) nos *pixels* da imagem original para facilitar a visualiza√ß√£o desta borda (o *cast* para `uint16` foi feito para evitar *overflow*)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFLGfUgwcuYB"},"outputs":[],"source":["new_camera_matrix, roi =\\\n","cv2.getOptimalNewCameraMatrix(camera_matrix, dist_coeffs, image_grayscale.shape[::-1], 1)\n","offset = 60\n","image_with_offset = np.clip(image.astype(np.uint16) + offset, 0, 255).astype(np.uint8)\n","undistorted_image = cv2.undistort(image_with_offset, camera_matrix, dist_coeffs, None, new_camera_matrix)\n","print(undistorted_image.shape)\n","Image.fromarray(undistorted_image)"]},{"cell_type":"markdown","metadata":{"id":"oyFmPcn1cuYB"},"source":["Vamos verificar a reproje√ß√£o dos pontos do tabuleiro em uma imagem, isto √©, sabendo as posi√ß√µes 3D dos pontos no sistema de refer√™ncia do tabuleiro `vertices3d`, e sabendo a pose do tabuleiro `rvecs[1]` e `tvecs[1]`, al√©m do √≠ntr√≠nseco da c√¢mera `camera_matrix` e `dist_coeffs`, fazemos esta proje√ß√£o do 3D do mundo para o 2D da c√¢mera."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDYHOAdTcuYC"},"outputs":[],"source":["projected_points, jacobian = cv2.projectPoints(vertices3d, rvecs[1], tvecs[1], camera_matrix, dist_coeffs)\n","image = cv2.cvtColor(cv2.imread(images_path + '/01.png', cv2.IMREAD_ANYDEPTH), cv2.COLOR_BAYER_GB2RGB)\n","for p in projected_points:\n","    cv2.circle(image, p.ravel().astype(np.int32), 6, (0, 255, 0), 2)\n","Image.fromarray(image)"]},{"cell_type":"markdown","metadata":{"id":"o9yG0-_4cuYC"},"source":["Na realidade, com o nosso tabuleiro em posi√ß√£o conhecida, podemos projetar os pontos que quisermos, veja por exemplo o eixo do sistema de refer√™ncia do tabuleiro, o eixo X √© o vermelho, o eixo verde √© o Y, e o azul √© o eixo Z. (Se n√£o prestar aten√ß√£o no BRG e RGB fica trocado o azul e o vermelho).\n","\n","Perceba que pela simetria, o sistema de coordenadas pode estar rotacionado de 180¬∫."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5LOSaTYcuYC"},"outputs":[],"source":["image = cv2.cvtColor(cv2.imread(images_path + '/01.png', cv2.IMREAD_ANYDEPTH), cv2.COLOR_BAYER_GB2BGR)\n","cv2.drawFrameAxes(image, camera_matrix, dist_coeffs, rvecs[1], tvecs[1], 0.5)\n","Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"]},{"cell_type":"markdown","metadata":{"id":"Wuz8VusKcuYC"},"source":["E agora se estiv√©ssemos em uma aplica√ß√£o de AR (Augmented Reality), poder√≠amos projetar uma geometria arbitr√°ria sobre o tabuleiro de xadrez. Vamos desenha um novo tabuleiro virtual verde afastado 0.15 m do original, isso apenas calculando a proje√ß√£o dos pontos deslocados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTJN-4iccuYC"},"outputs":[],"source":["image = cv2.cvtColor(cv2.imread(images_path + '/01.png', cv2.IMREAD_ANYDEPTH), cv2.COLOR_BAYER_GB2BGR)\n","reprojected_points1, _ = cv2.projectPoints(vertices3d, rvecs[1], tvecs[1], camera_matrix, dist_coeffs)\n","reprojected_points2, _ = cv2.projectPoints(vertices3d + np.array([[0, 0, -0.15]]), rvecs[1], tvecs[1], camera_matrix, dist_coeffs)\n","for p1, p2 in zip(reprojected_points1, reprojected_points2):\n","    cv2.line(image, p1.ravel().astype(np.int32), p2.ravel().astype(np.int32), (0, 255, 0), 2)\n","for i, p in enumerate(reprojected_points2):\n","    if i % num_vertices[0] != num_vertices[0] - 1:\n","        cv2.line(image, p.ravel().astype(np.int32), reprojected_points2[i + 1].ravel().astype(np.int32), (50, 200, 100), 2)\n","    if i < (num_vertices[1] - 1) * num_vertices[0]:\n","        cv2.line(image, p.ravel().astype(np.int32), reprojected_points2[i + num_vertices[0]].ravel().astype(np.int32), (50, 200, 100), 2)\n","Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"]},{"cell_type":"markdown","metadata":{"id":"DxGNXImjcuYC"},"source":["Agora que j√° realizamos a calibra√ß√£o e fizemos alguns experimentos, vamos implementar o nosso pr√≥prio `projectPoints` com o que j√° aprendemos\n","\n","Assim, primeiro voc√™ dever√° fazer a transforma√ß√£o de coordenadas (do referencial do objeto para o da c√¢mera), em seguida aplique a transforma√ß√£o projetiva para obter pontos 2D e finalmente aplique o modelo de distor√ß√£o, mapeando os pontos 2D em outros pontos 2D.\n","\n","A OpenCV usa o seguinte modelo de distor√ß√£o:\n","\n","\\begin{equation}\n","\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \n","\\mathbf{R}\n","\\begin{bmatrix} x_g \\\\ y_g \\\\ z_g \\end{bmatrix} + \\mathbf{t},\n","\\end{equation}\n","\n","\\begin{equation}\n","x' = x/z, \\\\\n","y' = y/z,\n","\\end{equation}\n","\n","\\begin{equation}\n","x'' = x' \\dfrac{1+k_1 r^2 + k_2 r^4 +k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2 (r^2 + 2x'^2), \\\\\n","y'' = y' \\dfrac{1+k_1 r^2 + k_2 r^4 +k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2p_2 x' y',\n","\\end{equation}\n","em que $r=\\sqrt{x'^2 + y'^2}$ e $(x'', y'')$ √© o valor do ponto distorcido. Assim, ap√≥s a distor√ß√£o, a transforma√ß√£o para o espa√ßo da imagem √© feita por\n","\\begin{equation}\n","u = f_x x'' + c_x, \\\\\n","v = f_y y'' + c_y.\n","\\end{equation}\n","\n","O vetor de distor√ß√µes √© definido (pelo OpenCV da seguinte forma):\n","\n","`dist_coeffs`=$(k_1 \\hspace{10pt} k_2 \\hspace{10pt} p_1 \\hspace{10pt} p_2 \\hspace{10pt} k_3)$\n","\n","Neste caso, considera-se que $k_4=k_5=k_6=0$."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"597b844298397ef44f2a1633dced930f","grade":false,"grade_id":"project_points_answer","locked":false,"schema_version":3,"solution":true,"task":false},"id":"k_H59Ww0cuYD"},"outputs":[],"source":["def project_points(object_points, rvec, tvec, camera_matrix, dist_coeffs):\n","    \"\"\"\n","    Your implementation of OpenCV's projectPoints() method.\n","    :param object_points: 3D points in format (N, 3) where N is the number of points.\n","    :param rvec: Rotation vector (Rodrigues convention), part of the extrinsic parameters.\n","    :param tvec: translation vector, part of the extrinsic parameters.\n","    :param camera_matrix: intrisic matrix.\n","    :param dist_coeffs: distortion coefficients following (k1, k2, p1, p2, k3).\n","    :return: projected 2D image points.\n","    \"\"\"\n","    fx = camera_matrix[0, 0]\n","    fy = camera_matrix[1, 1]\n","    cx = camera_matrix[0, 2]\n","    cy = camera_matrix[1, 2]\n","    k1, k2, p1, p2, k3 = dist_coeffs.ravel()\n","    rotation_matrix, _ = cv2.Rodrigues(rvec)\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","    return image_points"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"edcf33ceaa5b88789eb8323c9726393f","grade":true,"grade_id":"project_points_tests","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"HQ9T2KIVcuYD"},"outputs":[],"source":["points3d_test = np.arange(30, dtype=np.float32).reshape(-1, 3)\n","mtx_test = np.array([[1000, 0, 1000], [0, 1000, 1000], [0, 0, 1]], dtype=np.float32)\n","rot_test = Rotation.from_euler('XYZ', [45, 30, 60], degrees=True)\n","tvec_test = np.array([3, 4, 10], dtype=np.float32).reshape(-1, 1)\n","myproj_test = project_points(points3d_test, rot_test.as_rotvec(), tvec_test, mtx_test,  np.zeros(5))\n","cvproj_test, _ = cv2.projectPoints(points3d_test, rot_test.as_rotvec(), tvec_test, mtx_test,  np.zeros(5))\n","assert np.linalg.norm(myproj_test - cvproj_test.reshape(-1, 2)) < 1e-2"]},{"cell_type":"markdown","metadata":{"id":"ZfxLQit5cuYD"},"source":["As c√©lulas a seguir repetem as proje√ß√µes j√° realizadas anteriormente no tabuleiro, mas utilizando a fun√ß√£o ``my_project_points()`` ao inv√©s da fun√ß√£o ``cv2.projectPoints()``."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBn6A-fccuYD"},"outputs":[],"source":["projected_points = project_points(vertices3d, rvecs[1], tvecs[1], camera_matrix, dist_coeffs)\n","image = cv2.cvtColor(cv2.imread(images_path + '/01.png', cv2.IMREAD_ANYDEPTH), cv2.COLOR_BAYER_GB2RGB)\n","for p in projected_points:\n","    cv2.circle(image, p.ravel().astype(np.int32), 6, (0, 255, 0), 2)\n","Image.fromarray(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVxOPsukcuYD"},"outputs":[],"source":["image = cv2.cvtColor(cv2.imread(images_path + '/01.png', cv2.IMREAD_ANYDEPTH), cv2.COLOR_BAYER_GB2BGR)\n","reprojected_points1 = project_points(vertices3d, rvecs[1], tvecs[1], camera_matrix, dist_coeffs)\n","reprojected_points2 = project_points(vertices3d + np.array([[0, 0, -0.15]]), rvecs[1], tvecs[1], camera_matrix, dist_coeffs)\n","for p1, p2 in zip(reprojected_points1, reprojected_points2):\n","    cv2.line(image, p1.ravel().astype(np.int32), p2.ravel().astype(np.int32), (0, 255, 0), 2)\n","for i, p in enumerate(reprojected_points2):\n","    if i % num_vertices[0] != num_vertices[0] - 1:\n","        cv2.line(image, p.ravel().astype(np.int32), reprojected_points2[i + 1].ravel().astype(np.int32), (50, 200, 100), 2)\n","    if i < (num_vertices[1] - 1) * num_vertices[0]:\n","        cv2.line(image, p.ravel().astype(np.int32), reprojected_points2[i + num_vertices[0]].ravel().astype(np.int32), (50, 200, 100), 2)\n","Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"name":"lab1_cm203.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}